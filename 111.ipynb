{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "\n",
    "class MetroDataset(Dataset):\n",
    "    def __init__(self, grid_x_max, grid_y_max, exist_line_num, initial_station = None, static_size=2, dynamic_size=1):\n",
    "       \n",
    "\n",
    "        super(MetroDataset, self).__init__()\n",
    "\n",
    "        self.grid_x_max = grid_x_max\n",
    "        self.grid_y_max = grid_y_max\n",
    "        self.grid_index1_max = grid_y_max - 1 # The max index of the first dimension\n",
    "        self.grid_index2_max = grid_x_max - 1 # The max index of the second dimension\n",
    "        self.static_size = static_size\n",
    "        self.dynamic_size = dynamic_size\n",
    "\n",
    "        self.grid_num = grid_x_max*grid_y_max\n",
    "        self.exist_line_num = exist_line_num\n",
    "\n",
    "\n",
    "        self.positive = torch.Tensor([1]).long().to(device)\n",
    "        self.negative = torch.Tensor([-1]).long().to(device)\n",
    "        self.only1 = torch.tensor([1]).to(device)\n",
    "        self.sign = torch.Tensor([-1]).long().to(device)\n",
    "        \n",
    "        self.quadrant1 = torch.tensor([-1, 1]).view(1, 2).to(device) \n",
    "        self.quadrant2 = torch.tensor([-1, -1]).view(1, 2).to(device)  \n",
    "        self.quadrant3 = torch.tensor([1, -1]).view(1, 2).to(device) \n",
    "        self.quadrant4 = torch.tensor([1, 1]).view(1, 2).to(device)  \n",
    "        self.quadrant_up = torch.tensor([-1, 0]).view(1, 2).to(device)  \n",
    "        self.quadrant_right = torch.tensor([0, 1]).view(1, 2).to(device)  \n",
    "        self.quadrant_down = torch.tensor([1, 0]).view(1, 2).to(device)  \n",
    "        self.quadrant_left = torch.tensor([0, -1]).view(1, 2).to(device)  \n",
    "\n",
    "        a = []\n",
    "        self.null_tensor = torch.Tensor(a).long().to(device)  #tensor([], device='cuda:0', dtype=torch.int64) torch.Size([0])\n",
    "\n",
    "\n",
    "        # build dynamic\n",
    "        self.dynamic = torch.zeros((1, self.dynamic_size, self.grid_num)).float().to(device)# size with batch\n",
    "        if initial_station:\n",
    "            initial_station_vec = initial_station[0]*grid_x_max + initial_station[1]\n",
    "            self.dynamic[0, 0, initial_station_vec] = 1\n",
    "\n",
    "\n",
    "        #build static\n",
    "        def build_static(grid_x_max, grid_y_max):\n",
    "            for i in range(grid_y_max):\n",
    "                per_need = np.zeros((grid_x_max, 2))\n",
    "\n",
    "                for j in range(grid_x_max):\n",
    "                    per_need[j, 0] = i\n",
    "                    per_need[j, 1] = j\n",
    "\n",
    "                if i == 0:\n",
    "                    need = per_need\n",
    "                else:\n",
    "                    need = np.vstack((need, per_need))\n",
    "\n",
    "            np_static = need.transpose()\n",
    "\n",
    "            return np_static\n",
    "\n",
    "        np_static = build_static(grid_x_max, grid_y_max)\n",
    "        ten_static = torch.from_numpy(np_static).float()\n",
    "        self.static = ten_static.view(1, self.static_size, self.grid_num).to(device) # size with batch\n",
    "\n",
    "\n",
    " ########## build existing lines\n",
    "        def g_to_v1(agent_grids):\n",
    "\n",
    "            vector_index = agent_grids[:, 0] * grid_x_max + agent_grids[:, 1]\n",
    "\n",
    "            return vector_index\n",
    "\n",
    "        def process_line(index_line, exi_sta_adj_sta):\n",
    "            \n",
    "            for i in range(len(index_line)):\n",
    "                this_index = index_line[i]\n",
    "                if this_index not in exi_sta_adj_sta:\n",
    "                    exi_sta_adj_sta[this_index] = []\n",
    "\n",
    "                if (i - 2) >= 0:\n",
    "                    exi_sta_adj_sta[this_index].append(index_line[i - 2])\n",
    "                if (i - 1) >= 0:\n",
    "                    exi_sta_adj_sta[this_index].append(index_line[i - 1])\n",
    "                if (i + 1) <= (len(index_line) - 1):\n",
    "                    exi_sta_adj_sta[this_index].append(index_line[i + 1])\n",
    "                if (i + 2) <= (len(index_line) - 1):\n",
    "                    exi_sta_adj_sta[this_index].append(index_line[i + 2])\n",
    "#########################################################################################################\n",
    "        # original representation of each line\n",
    "        line0_ststion_list = [[8, 2], [10, 3], [10, 5], [11, 6], [11, 7], [12, 9], [12, 11], [12, 12], [12, 13],\n",
    "                              [12, 14], [12, 15], [12, 16], [12, 17], [12, 18], [12, 20], [11, 22], [11, 23], [11, 24],\n",
    "                              [11, 25]]\n",
    "        line1_ststion_list = [[0, 13], [1, 14], [3, 14], [4, 14], [5, 14], [6, 14], [8, 14], [9, 14], [10, 14],\n",
    "                              [12, 14], [13, 14], [14, 14], [15, 14], [16, 14], [17, 14], [18, 14], [20, 14], [21, 14],\n",
    "                              [23, 14], [24, 14], [26, 14]]\n",
    "\n",
    "        # line2_ststion_list = [[15, 5], [15, 7], [15, 9], [16, 10], [17, 11], [17, 13], [17, 14], [17, 16], [17, 17],\n",
    "        #                       [16, 18], [15, 19], [14, 19], [13, 19], [11, 18], [9, 18], [8, 18], [7, 20], [7, 22], [7, 23],\n",
    "        #                       [5, 24], [3, 23], [2, 22], [0, 23]]\n",
    "        #\n",
    "        # line3_ststion_list = [[0, 13], [1, 12], [2, 12], [3, 12], [4, 13], [4, 14], [4, 15], [4, 16], [5, 16], [6, 16],\n",
    "        #                       [7, 16], [9, 16], [10, 16], [11, 16], [12, 15], [13, 15], [14, 15], [15, 16], [16, 16], [17, 16],\n",
    "        #                       [18, 16], [20, 17], [22, 17], [23, 17], [24, 17], [25, 17], [26, 17], [26, 19], [26, 20]]\n",
    "\n",
    "        ########################## the first step for add new lines, the total need 4 steps\n",
    "        # need used the below function exlude_od_pair(grid_x_max) to exclude the od pair alone added line\n",
    "\n",
    "\n",
    "        # att3_5\n",
    "        # index_line4_station = [815, 759, 730, 701, 672, 644, 615, 587, 559, 531, 503, 504, 476, 477, 448, 419, 391, 392, 393, 365,\n",
    "        #                        337, 308, 279, 250, 222, 223, 224,225, 197, 168, 140, 112]\n",
    "\n",
    "\n",
    "\n",
    "        ##########################\n",
    "        np_line0_station = np.array(line0_ststion_list)\n",
    "        np_line1_station = np.array(line1_ststion_list)\n",
    "        # np_line2_station = np.array(line2_ststion_list)\n",
    "        # np_line3_station = np.array(line3_ststion_list)\n",
    "\n",
    "        index_line0_station = g_to_v1(np_line0_station)\n",
    "        index_line1_station = g_to_v1(np_line1_station)\n",
    "        # index_line2_station = g_to_v1(np_line2_station)\n",
    "        # index_line3_station = g_to_v1(np_line3_station)\n",
    "\n",
    "        index_line0_station = [int(i) for i in index_line0_station]\n",
    "        index_line1_station = [int(i) for i in index_line1_station]\n",
    "        # index_line2_station = [int(i) for i in index_line2_station]\n",
    "        # index_line3_station = [int(i) for i in index_line3_station]\n",
    "\n",
    "        index_line_station_list = []\n",
    "        index_line_station_list.append(index_line0_station)\n",
    "        index_line_station_list.append(index_line1_station)\n",
    "        # index_line_station_list.append(index_line2_station)\n",
    "        # index_line_station_list.append(index_line3_station)\n",
    "\n",
    "        ####################### the second step for add new lines, the total need 4 steps\n",
    "        # index_line_station_list.append(index_line4_station)\n",
    "        # index_line_station_list.append(index_line5_station)\n",
    "        # index_line_station_list.append(index_line6_station)\n",
    "        # index_line_station_list.append(index_line7_station)\n",
    "        # index_line_station_list.append(index_line8_station)\n",
    "\n",
    "        self.line_station_list = index_line_station_list\n",
    "\n",
    "##################build full cross grid including the grids which have no station\n",
    "\n",
    "        line0_full_list = [[8, 2], [10, 3], [10, 4], [10, 5], [11, 6], [11, 7], [12, 9], [12, 10],[12, 11], [12, 12], [12, 13],\n",
    "                           [12, 14], [12, 15], [12, 16], [12, 17], [12, 18], [12, 19], [12, 20], [11, 22], [11, 23], [11, 24],\n",
    "                           [11, 25]]\n",
    "\n",
    "        line1_full_list = [[0, 13], [1, 14], [2, 14], [3, 14], [4, 14], [5, 14], [6, 14],[7, 14], [8, 14], [9, 14], [10, 14],[11, 14],\n",
    "                           [12, 14], [13, 14], [14, 14], [15, 14], [16, 14], [17, 14], [18, 14],[19, 14], [20, 14], [21, 14],[22, 14],\n",
    "                           [23, 14], [24, 14],[25, 14], [26, 14]]\n",
    "        #\n",
    "        # line2_full_list = [[15, 5], [15, 6], [15, 7], [15, 8], [15, 9], [16, 10], [17, 11], [17, 12], [17, 13], [17, 14], [17, 15], [17, 16], [17, 17],\n",
    "        #                    [16, 18], [15, 19], [14, 19], [13, 19], [11, 18], [10, 18], [9, 18], [8, 18], [7, 20], [7, 21], [7, 22], [7, 23],\n",
    "        #                    [5, 24], [3, 23], [2, 22], [0, 23]]\n",
    "        #\n",
    "        # line3_full_list = [[0, 13], [1, 12], [2, 12], [3, 12], [4, 13], [4, 14], [4, 15], [4, 16], [5, 16], [6, 16],\n",
    "        #                    [7, 16], [8, 16], [9, 16], [10, 16], [11, 16], [12, 15], [13, 15], [14, 15], [15, 16], [16, 16], [17, 16],\n",
    "        #                    [18, 16], [20, 17], [21, 17], [22, 17], [23, 17], [24, 17], [25, 17], [26, 17], [26, 18], [26, 19], [26, 20]]\n",
    "\n",
    "        np_line0_full = np.array(line0_full_list)\n",
    "        np_line1_full = np.array(line1_full_list)\n",
    "        # np_line2_full = np.array(line2_full_list)\n",
    "        # np_line3_full = np.array(line3_full_list)\n",
    "\n",
    "        index_line0_full = g_to_v1(np_line0_full)\n",
    "        index_line1_full = g_to_v1(np_line1_full)\n",
    "        # index_line2_full = g_to_v1(np_line2_full)\n",
    "        # index_line3_full = g_to_v1(np_line3_full)\n",
    "\n",
    "        index_line0_full = [int(i) for i in index_line0_full]\n",
    "        index_line1_full = [int(i) for i in index_line1_full]\n",
    "        # index_line2_full = [int(i) for i in index_line2_full]\n",
    "        # index_line3_full = [int(i) for i in index_line3_full]\n",
    "\n",
    "        index_line_full_list = []\n",
    "        index_line_full_list.append(index_line0_full)\n",
    "        index_line_full_list.append(index_line1_full)\n",
    "        # index_line_full_list.append(index_line2_full)\n",
    "        # index_line_full_list.append(index_line3_full)\n",
    "\n",
    "        ## the third step for add new, the total need 4 steps\n",
    "        # index_line_full_list.append(index_line4_station)\n",
    "        # index_line_full_list.append(index_line5_station)\n",
    "        # index_line_full_list.append(index_line6_station)\n",
    "        # index_line_full_list.append(index_line7_station)\n",
    "        # index_line_full_list.append(index_line8_station)\n",
    "        ###\n",
    "        exi_sta_adj_sta = {}\n",
    "        index_line_list = index_line_full_list\n",
    "\n",
    "        for j in index_line_list:\n",
    "            process_line(j, exi_sta_adj_sta)\n",
    "\n",
    "        # qu chong\n",
    "        for key, value in exi_sta_adj_sta.items():\n",
    "            value = list(set(value))\n",
    "            exi_sta_adj_sta[key] = value\n",
    "\n",
    "        self.exi_sta_adj_sta = exi_sta_adj_sta\n",
    "\n",
    "        # line_full_tensor\n",
    "        self.line_full_tensor = []\n",
    "###############\n",
    "        # CPU\n",
    "        line_full_tensor0 = torch.tensor(index_line0_full).view(len(line0_full_list), 1)\n",
    "        line_full_tensor1 = torch.tensor(index_line1_full).view(len(line1_full_list), 1)\n",
    "        # line_full_tensor2 = torch.tensor(index_line2_full).view(len(line2_full_list), 1)\n",
    "        # line_full_tensor3 = torch.tensor(index_line3_full).view(len(line3_full_list), 1)\n",
    "\n",
    "        self.line_full_tensor.append(line_full_tensor0)\n",
    "        self.line_full_tensor.append(line_full_tensor1)\n",
    "        # self.line_full_tensor.append(line_full_tensor2)\n",
    "        # self.line_full_tensor.append(line_full_tensor3)\n",
    "\n",
    "        #################### the fourth step for add new lines, the total need 4 steps\n",
    "        # line_full_tensor4 = torch.tensor(index_line4_station).view(len(index_line4_station), 1)\n",
    "        # self.line_full_tensor.append(line_full_tensor4)\n",
    "        # #\n",
    "        # line_full_tensor5 = torch.tensor(index_line5_station).view(len(index_line5_station), 1)\n",
    "        # self.line_full_tensor.append(line_full_tensor5)\n",
    "        # # #\n",
    "        # line_full_tensor6 = torch.tensor(index_line6_station).view(len(index_line6_station), 1)\n",
    "        # self.line_full_tensor.append(line_full_tensor6)\n",
    "        #\n",
    "        # line_full_tensor7 = torch.tensor(index_line7_station).view(len(index_line7_station), 1)\n",
    "        # self.line_full_tensor.append(line_full_tensor7)\n",
    "        #\n",
    "        # line_full_tensor8 = torch.tensor(index_line8_station).view(len(index_line8_station), 1)\n",
    "        # self.line_full_tensor.append(line_full_tensor8)\n",
    "#####################\n",
    "        # GPU\n",
    "        # line_full_tensor0 = torch.tensor(index_line0_full).view(len(line0_full_list), 1).to(device)\n",
    "        # line_full_tensor1 = torch.tensor(index_line1_full).view(len(line1_full_list), 1).to(device)\n",
    "        # line_full_tensor2 = torch.tensor(index_line2_full).view(len(line2_full_list), 1).to(device)\n",
    "        # line_full_tensor3 = torch.tensor(index_line3_full).view(len(line3_full_list), 1).to(device)\n",
    "        #\n",
    "        # self.line_full_tensor.append(line_full_tensor0)\n",
    "        # self.line_full_tensor.append(line_full_tensor1)\n",
    "        # self.line_full_tensor.append(line_full_tensor2)\n",
    "        # self.line_full_tensor.append(line_full_tensor3)\n",
    "        #\n",
    "        # #################### the fourth step for add new lines, the total need 4 steps\n",
    "        # line_full_tensor4 = torch.tensor(index_line4_station).view(len(index_line4_station), 1).to(device)\n",
    "        # self.line_full_tensor.append(line_full_tensor4)\n",
    "        #\n",
    "        # line_full_tensor5 = torch.tensor(index_line5_station).view(len(index_line5_station), 1).to(device)\n",
    "        # self.line_full_tensor.append(line_full_tensor5)\n",
    "################################\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "        def increment_1():\n",
    "            #output-- grid_inc_1: CUDA size (9,2)\n",
    "            grid_list = [[-2,0],[-2,1],[-2,2],[-1,0],[-1,1],[-1,2],[0,1],[0,2]]\n",
    "\n",
    "            grid_inc_1 = torch.tensor(grid_list).to(device)\n",
    "\n",
    "            return grid_inc_1  # CUDA  torch.Size([8, 2]) dtype: torch.int64\n",
    "\n",
    "        def increment_2():\n",
    "            \n",
    "            #output-- grid_inc_2: CUDA size (9,2)\n",
    "            grid_list = [[-2,-2],[-2,-1],[-2,0],[-1,-2],[-1,-1],[-1,0],[0,-2],[0,-1]]\n",
    "\n",
    "            grid_inc_2 = torch.tensor(grid_list).to(device)\n",
    "\n",
    "            return grid_inc_2  # CUDA  torch.Size([8, 2]) dtype: torch.int64\n",
    "\n",
    "        def increment_3():\n",
    "            \n",
    "            #output-- grid_inc_2: CUDA size (9,2)\n",
    "            grid_list = [[0,-2],[0,-1],[1,-2],[1,-1],[1,0],[2,-2],[2,-1],[2,0]]\n",
    "\n",
    "            grid_inc_3 = torch.tensor(grid_list).to(device)\n",
    "\n",
    "            return grid_inc_3  # CUDA  torch.Size([8, 2]) dtype: torch.int64\n",
    "\n",
    "        def increment_4():\n",
    "            \n",
    "            #output-- grid_inc_2: CUDA size (9,2)\n",
    "            grid_list = [[0,1],[0,2],[1,0],[1,1],[1,2],[2,0],[2,1],[2,2]]\n",
    "\n",
    "            grid_inc_4 = torch.tensor(grid_list).to(device)\n",
    "\n",
    "            return grid_inc_4  # CUDA  torch.Size([8, 2]) dtype: torch.int64\n",
    "\n",
    "        def increment_up():\n",
    "            \n",
    "            #output-- grid_inc_up: CUDA size (9,2)\n",
    "            grid_list = [[-2,-2],[-2,-1],[-2,0],[-2,1],[-2,2],[-1,-2],[-1,-1],[-1,0],[-1,1],[-1,2],[0,-2],[0,-1],[0,1],[0,2]]\n",
    "\n",
    "            grid_inc_up = torch.tensor(grid_list).to(device)\n",
    "\n",
    "            return grid_inc_up  # CUDA  torch.Size([14, 2]) dtype: torch.int64\n",
    "\n",
    "        def increment_right():\n",
    "            \n",
    "            #output-- grid_inc_up: CUDA size (9,2)\n",
    "            grid_list = [[-2,0],[-2,1],[-2,2],[-1,0],[-1,1],[-1,2],[0,1],[0,2],[1,0],[1,1],[1,2],[2,0],[2,1],[2,2]]\n",
    "\n",
    "            grid_inc_right = torch.tensor(grid_list).to(device)\n",
    "\n",
    "            return grid_inc_right  # CUDA  torch.Size([14, 2]) dtype: torch.int64\n",
    "\n",
    "        def increment_down():\n",
    "            \n",
    "            #output-- grid_inc_up: CUDA size (9,2)\n",
    "            grid_list = [[0,-2],[0,-1],[0,1],[0,2],[1,-2],[1,-1],[1,0],[1,1],[1,2],[2,-2],[2,-1],[2,0],[2,1],[2,2]]\n",
    "\n",
    "            grid_inc_down = torch.tensor(grid_list).to(device)\n",
    "\n",
    "            return grid_inc_down # CUDA  torch.Size([14, 2]) dtype: torch.int64\n",
    "\n",
    "        def increment_left():\n",
    "            \n",
    "            #output-- grid_inc_up: CUDA size (9,2)\n",
    "            grid_list = [[-2,-2],[-2,-1],[-2,0],[-1,-2],[-1,-1],[-1,0],[0,-2],[0,-1],[1,-2],[1,-1],[1,0],[2,-2],[2,-1],[2,0]]\n",
    "\n",
    "            grid_inc_left = torch.tensor(grid_list).to(device)\n",
    "\n",
    "            return grid_inc_left # CUDA  torch.Size([14, 2]) dtype: torch.int64\n",
    "\n",
    "        def increment_full():\n",
    "            #output-- grid_inc_full: CUDA size (24,2)\n",
    "            grid_list = [[-2,-2],[-2,-1],[-2,0],[-2,1],[-2,2],[-1,-2],[-1,-1],[-1,0],[-1,1],[-1,2],[0,-2],[0,-1],[0,1],[0,2],\n",
    "                         [1,-2],[1,-1],[1,0],[1,1],[1,2],[2,-2],[2,-1],[2,0],[2,1],[2,2]]\n",
    "\n",
    "            grid_inc_full = torch.tensor(grid_list).to(device)\n",
    "\n",
    "            return grid_inc_full # CUDA  torch.Size([24, 2]) dtype: torch.int64\n",
    "\n",
    "        self.grid_inc_1 = increment_1()\n",
    "        self.grid_inc_2 = increment_2()\n",
    "        self.grid_inc_3 = increment_3()\n",
    "        self.grid_inc_4 = increment_4()\n",
    "        self.grid_inc_up = increment_up()\n",
    "        self.grid_inc_right = increment_right()\n",
    "        self.grid_inc_down = increment_down()\n",
    "        self.grid_inc_left = increment_left()\n",
    "        self.grid_inc_full = increment_full()\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "    def v_to_g(self, index):\n",
    "        \n",
    "\n",
    "        grid_x = index // self.grid_x_max\n",
    "        grid_y = index % self.grid_x_max\n",
    "\n",
    "        grid_x1 = grid_x.view(1)\n",
    "        grid_y1 = grid_y.view(1)\n",
    "\n",
    "        grid_index = torch.cat((grid_x1, grid_y1), dim=0)\n",
    "        grid_index1 = grid_index.view(1, 2)\n",
    "\n",
    "        return grid_index1\n",
    "\n",
    "    def agent_direct_vector(self, direction_vector, grid_index1, exist_agent_last_grid):\n",
    "        \n",
    "\n",
    "        grid_deviation = grid_index1 - exist_agent_last_grid  # CUDA  torch.Size([1, 2])  dtype: torch.int64\n",
    "\n",
    "        deviation1 = torch.where(grid_deviation > 0, self.positive, grid_deviation)\n",
    "\n",
    "        deviation2 = torch.where(deviation1 < 0, self.negative, deviation1)\n",
    "\n",
    "        if torch.equal(deviation2, self.quadrant1):\n",
    "            direction_vector[0, 1] = 1\n",
    "        elif torch.equal(deviation2, self.quadrant2):\n",
    "            direction_vector[0,  7] = 1\n",
    "        elif torch.equal(deviation2, self.quadrant3):\n",
    "            direction_vector[0, 5] = 1\n",
    "        elif torch.equal(deviation2, self.quadrant4):\n",
    "            direction_vector[0, 3] = 1\n",
    "        elif torch.equal(deviation2, self.quadrant_up):\n",
    "            direction_vector[0, 0] = 1\n",
    "        elif torch.equal(deviation2, self.quadrant_right):\n",
    "            direction_vector[0, 2] = 1\n",
    "        elif torch.equal(deviation2, self.quadrant_down):\n",
    "            direction_vector[0, 4] = 1\n",
    "        elif torch.equal(deviation2, self.quadrant_left):\n",
    "            direction_vector[0, 6] = 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        return direction_vector  # CUDA   torch.Size([1, 8])   torch.int64\n",
    "\n",
    "    def agent_direct_control(self, direction_vector):\n",
    "      \n",
    "\n",
    "        allow_direction = torch.zeros((1, 8)).long().to(device)\n",
    "       \n",
    "\n",
    "        if (direction_vector[0,1] == 1) or (direction_vector[0,0] == 1 and direction_vector[0,2] == 1):\n",
    "            allow_direction[0, 1] = 1\n",
    "\n",
    "        elif (direction_vector[0,7] == 1) or (direction_vector[0,0] == 1 and direction_vector[0,6] == 1):\n",
    "            allow_direction[0, 7] = 1\n",
    "            \n",
    "\n",
    "        elif (direction_vector[0, 5] == 1) or (direction_vector[0, 4] == 1 and direction_vector[0, 6] == 1):\n",
    "            allow_direction[0, 5] = 1\n",
    "            \n",
    "\n",
    "        elif (direction_vector[0, 3] == 1) or (direction_vector[0, 2] == 1 and direction_vector[0, 4] == 1):\n",
    "            allow_direction[0, 3] = 1\n",
    "            \n",
    "\n",
    "        elif (direction_vector[0, 0] == 1) and (torch.sum(direction_vector).view(1) == self.only1):\n",
    "            allow_direction[0, 0] = 1\n",
    "            \n",
    "\n",
    "        elif (direction_vector[0, 2] == 1) and (torch.sum(direction_vector).view(1) == self.only1):\n",
    "            allow_direction[0, 2] = 1\n",
    "            \n",
    "\n",
    "        elif (direction_vector[0, 4] == 1) and (torch.sum(direction_vector).view(1) == self.only1):\n",
    "            allow_direction[0, 4] = 1\n",
    "           \n",
    "\n",
    "        elif (direction_vector[0, 6] == 1) and (torch.sum(direction_vector).view(1) == self.only1):\n",
    "            allow_direction[0, 6] = 1\n",
    "            \n",
    "        else:\n",
    "            pass \n",
    "        return allow_direction\n",
    "\n",
    "\n",
    "    def optional_grids1(self, grid_index1, allow_direction):\n",
    "        \n",
    "\n",
    "        if allow_direction[0, 0] == 1:\n",
    "            grids_allow0 = grid_index1 + self.grid_inc_up\n",
    "        elif allow_direction[0, 1] == 1:\n",
    "            grids_allow0 = grid_index1 + self.grid_inc_1\n",
    "        elif allow_direction[0, 2] == 1:\n",
    "            grids_allow0 = grid_index1 + self.grid_inc_right\n",
    "        elif allow_direction[0, 3] == 1:\n",
    "            grids_allow0 = grid_index1 + self.grid_inc_4\n",
    "        elif allow_direction[0, 4] == 1:\n",
    "            grids_allow0 = grid_index1 + self.grid_inc_down\n",
    "        elif allow_direction[0, 5] == 1:\n",
    "            grids_allow0 = grid_index1 + self.grid_inc_3\n",
    "        elif allow_direction[0, 6] == 1:\n",
    "            grids_allow0 = grid_index1 + self.grid_inc_left\n",
    "        elif allow_direction[0, 7] == 1:\n",
    "            grids_allow0 = grid_index1 + self.grid_inc_2\n",
    "        else:\n",
    "            \n",
    "            grids_allow0 = grid_index1 + self.grid_inc_full\n",
    "\n",
    "        \n",
    "\n",
    "        grid_index2_max = self.grid_index2_max  \n",
    "\n",
    "        grids_allow_sign0 = torch.where(grids_allow0 <= grid_index2_max, grids_allow0, self.sign)\n",
    "\n",
    "        grids_allow_sign = torch.where(grids_allow_sign0 < 0, self.sign, grids_allow_sign0)\n",
    "\n",
    "        area1 = (grids_allow_sign[:, 0] != -1) & (grids_allow_sign[:, 1] != -1)\n",
    "        \n",
    "        grids_allow = grids_allow0[area1] \n",
    "        \n",
    "        return grids_allow\n",
    "\n",
    "    def g_to_v(self, agent_grids):  # need to change with input as CUDE Tensor\n",
    "       \n",
    "\n",
    "        vector_index = agent_grids[:, 0] * self.grid_x_max + agent_grids[:, 1]\n",
    "\n",
    "        return vector_index\n",
    "\n",
    "   \n",
    "    def exi_line_control(self, agent_current_index, vector_index):\n",
    "        \n",
    "\n",
    "        if self.exi_sta_adj_sta == None:\n",
    "            vector_index_allow = vector_index\n",
    "        else:\n",
    "            try: \n",
    "                grid_exi_mask = self.exi_sta_adj_sta[agent_current_index]  \n",
    "                #print('grid_exi_mask:',grid_exi_mask)\n",
    "                #print('grid_exi_mask[0]_type:',type(grid_exi_mask[0]))\n",
    "            except:\n",
    "                vector_index_allow = vector_index\n",
    "            else:\n",
    "                num = 0\n",
    "                for i in grid_exi_mask:\n",
    "                    num = num + 1\n",
    "                    this_area = (vector_index[:] != i)\n",
    "\n",
    "                    if num == 1:\n",
    "                        area = this_area\n",
    "                    else:\n",
    "                        area = area & this_area\n",
    "                vector_index_allow = vector_index[area]  #，vector_index_allow=tensor([], device='cuda:0', dtype=torch.int64)\n",
    "\n",
    "        return vector_index_allow\n",
    "\n",
    "\n",
    "    # add vector_index_allow to 1 vector mask\n",
    "\n",
    "    def vector_allow(self, agent_current_index, grid_index1, exist_agent_last_grid, direction_vector):\n",
    "        \n",
    "        # output--direction_vector:\n",
    "        #         vector_index_allow:  CUDA,\n",
    "        #         example1: tensor([2, 3], device='cuda:0')--agent can choose 2 and 3 grids.\n",
    "        #         example2: tensor([], device='cuda:0')--agent can choose no grids.\n",
    "\n",
    "        direction_vector = self.agent_direct_vector(direction_vector, grid_index1, exist_agent_last_grid)\n",
    "\n",
    "        allow_direction = self.agent_direct_control(direction_vector)\n",
    "\n",
    "        grids_allow = self.optional_grids1(grid_index1, allow_direction)\n",
    "\n",
    "        if grids_allow.size()[0]: \n",
    "\n",
    "            vector_index = self.g_to_v(grids_allow)\n",
    "\n",
    "            vector_index_allow = self.exi_line_control(agent_current_index, vector_index) #\n",
    "\n",
    "            if not vector_index_allow.size()[0]:  #vector_index_allow: tensor([], device='cuda:0', dtype=torch.int64)\n",
    "                vector_index_allow = self.null_tensor\n",
    "\n",
    "        else: # grids_allow =  tensor([], device='cuda:0', dtype=torch.int64) \n",
    "\n",
    "\n",
    "            vector_index_allow = self.null_tensor\n",
    "\n",
    "        return direction_vector, vector_index_allow\n",
    "\n",
    "\n",
    "    def update_mask(self, vector_index_allow): # focuse  CUDA Tensor\n",
    "        \n",
    "\n",
    "        # output-- mask:  CUDA, torch.Size([1, city_number]) torch.float32\n",
    "\n",
    "        mask_initial = torch.zeros(1, self.grid_num, device=device).long() # 1 : bacth_size\n",
    "\n",
    "\n",
    "        mask = mask_initial.index_fill_(1, vector_index_allow, 1).float()  # the first 1: dim , the second 1: value\n",
    "\n",
    "        #mask: example--tensor([[0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.]],device='cuda:0')\n",
    "        # size: torch.Size([1, 16])  .dtype: torch.float32\n",
    "        return mask\n",
    "\n",
    "    def update_dynamic(self, dynamic, agent_current_index):\n",
    "       \n",
    "\n",
    "        h = float(1)\n",
    "\n",
    "\n",
    "        dynamic = dynamic.clone()\n",
    "        dynamic[0, 0, agent_current_index] = h\n",
    "\n",
    "        return dynamic\n",
    "####################################################################\n",
    "# define reward\n",
    "\n",
    "##########\n",
    "#build od matrix\n",
    "\n",
    "# od_path =r'/home/weiyu/program/metro_expand_combination/OD.txt'\n",
    "\n",
    "def local_g_to_v(grid, grid_x_max):\n",
    "    # grid: axample-- 0,0  string\n",
    "    # grid_x_max：\n",
    "    grid_x, grid_y = grid.split(',')\n",
    "\n",
    "    index = int(grid_x)*grid_x_max + int(grid_y)\n",
    "    index1 =str(index)\n",
    "\n",
    "    return index1\n",
    "\n",
    "def index_od(od_path, grid_x_max, od_index_path):\n",
    "\n",
    "    f = open(od_path, 'r')\n",
    "    m = open(od_index_path, 'w')\n",
    "\n",
    "    for line in f:\n",
    "        grid1, grid2, weight = line.rstrip().split('\\t')\n",
    "\n",
    "        index1 = local_g_to_v(grid1, grid_x_max)\n",
    "        index2 = local_g_to_v(grid2, grid_x_max)\n",
    "\n",
    "        to_write = index1+'\\t'+index2+'\\t'+weight+'\\n'\n",
    "\n",
    "        m.write(to_write)\n",
    "    m.close()\n",
    "    f.close()\n",
    "\n",
    "#GPU\n",
    "def build_od_matrix(grid_num, od_index_path):\n",
    "    \n",
    "    od_matirx = torch.zeros((grid_num, grid_num)).to(device)\n",
    "\n",
    "    f = open(od_index_path, 'r')\n",
    "    for line in f:\n",
    "        index1, index2, weight = line.rstrip().split('\\t')\n",
    "        index11 = int(index1)\n",
    "        index21 = int(index2)\n",
    "        weight1 = float(weight)\n",
    "\n",
    "        od_matirx[index11][index21] = weight1\n",
    "    f.close()\n",
    "\n",
    "    return od_matirx\n",
    "\n",
    "# od_matirx = build_od_matrix(grid_num, od_index_path)\n",
    "\n",
    "def agent_pair(agent_grid_list):\n",
    "   \n",
    "    # output--satisfied_od_pair:   [[1,2],[2,3]]\n",
    "\n",
    "    satisfied_od_pair = []\n",
    "\n",
    "    for i in range(len(agent_grid_list) - 1):\n",
    "        for j in range(i + 1, len(agent_grid_list)):\n",
    "            per_od_pair = []\n",
    "            per_od_pair.append(agent_grid_list[i])\n",
    "            per_od_pair.append(agent_grid_list[j])\n",
    "            satisfied_od_pair.append(per_od_pair)\n",
    "\n",
    "    return satisfied_od_pair\n",
    "\n",
    "#GPU\n",
    "def agent_exist_line_pair(tour_idx, agent_grid_list, per_line_full_tensor, per_line_station_list):\n",
    "   \n",
    "\n",
    "    satisfied_od_pair = []\n",
    "\n",
    "    agent_line = (tour_idx - per_line_full_tensor)\n",
    "\n",
    "    intersection_need = (agent_line == 0).nonzero()\n",
    "\n",
    "    if intersection_need.size()[0] == 0:\n",
    "        pass # there is no interaction\n",
    "\n",
    "    else:\n",
    "        interaction_index_mult = intersection_need[:, 1]\n",
    "        interaction_index_list = []\n",
    "        for i in interaction_index_mult:\n",
    "            interaction_index_list.append(agent_grid_list[i])\n",
    "\n",
    "        for i in agent_grid_list:\n",
    "            if i not in interaction_index_list:\n",
    "                for j in per_line_station_list:\n",
    "                    if j not in interaction_index_list:\n",
    "                        per_od_pair = []\n",
    "                        per_od_pair.append(i)\n",
    "                        per_od_pair.append(j)\n",
    "                        satisfied_od_pair.append(per_od_pair)\n",
    "\n",
    "    return satisfied_od_pair # for each element: the agent station is the first\n",
    "\n",
    "# GPU\n",
    "def satisfied_od_pair_fn(tour_idx, agent_grid_list, line_full_tensor, line_station_list, exist_line_num):\n",
    "    \n",
    "\n",
    "    #agent_station_num = len(agent_grid_list)\n",
    "\n",
    "    satisfied_od_pair1 = agent_pair(agent_grid_list)\n",
    "\n",
    "    satisfied_od_pair2 = []\n",
    "\n",
    "    for i in range(exist_line_num):\n",
    "        per_line_full_tensor = line_full_tensor[i]\n",
    "\n",
    "        per_line_station_list = line_station_list[i]\n",
    "\n",
    "        per_satisfied_od_pair2 = agent_exist_line_pair(tour_idx, agent_grid_list, per_line_full_tensor, per_line_station_list)\n",
    "        \n",
    "\n",
    "        satisfied_od_pair2 = satisfied_od_pair2 + per_satisfied_od_pair2\n",
    "\n",
    "    satisfied_od_pair = satisfied_od_pair1 + satisfied_od_pair2\n",
    "\n",
    "    return satisfied_od_pair  # list cpu\n",
    "\n",
    "#GPU\n",
    "def satisfied_od_mask_fn(grid_num, satisfied_od_pair):\n",
    "    # build the satisfied_od_mask: the element 1(0) present this od is (is not) satisfied by the agent\n",
    "\n",
    "    satisfied_od_mask = torch.zeros(grid_num, grid_num).byte().to(device)  # initial########################\n",
    "\n",
    "    value = torch.tensor([1]).byte().to(device)\n",
    "\n",
    "    for per_pair in satisfied_od_pair:\n",
    "        i,j = per_pair\n",
    "\n",
    "        satisfied_od_mask[i][j] = value\n",
    "\n",
    "    return satisfied_od_mask\n",
    "\n",
    "\n",
    "# GPU\n",
    "def reward_fn(tour_idx, grid_num, agent_grid_list, line_full_tensor, line_station_list, exist_line_num, od_matirx):\n",
    "\n",
    "    satisfied_od_pair = satisfied_od_pair_fn(tour_idx, agent_grid_list, line_full_tensor, line_station_list, exist_line_num)\n",
    "    # up ok\n",
    "    satisfied_od_mask = satisfied_od_mask_fn(grid_num, satisfied_od_pair)\n",
    "\n",
    "    satisfied_od_tensor = torch.masked_select(od_matirx, satisfied_od_mask)\n",
    "\n",
    "    reward = satisfied_od_tensor.sum()   # CUDA,\n",
    "\n",
    "    return reward\n",
    "\n",
    "#########reward cpu\n",
    "#CPU\n",
    "def build_od_matrix1(grid_num, od_index_path):\n",
    "    \n",
    "    od_matirx = torch.zeros((grid_num, grid_num))\n",
    "\n",
    "    f = open(od_index_path, 'r')\n",
    "    for line in f:\n",
    "        index1, index2, weight = line.rstrip().split('\\t')\n",
    "        index11 = int(index1)\n",
    "        index21 = int(index2)\n",
    "        weight1 = float(weight)\n",
    "\n",
    "        od_matirx[index11][index21] = weight1\n",
    "    f.close()\n",
    "\n",
    "    return od_matirx\n",
    "\n",
    "##CPU\n",
    "## This part is used to ecxclude od pair\n",
    "\n",
    "def process_segment(per_segment,grid_x_max):\n",
    "\n",
    "    per_seg_ind = []\n",
    "    for i in per_segment:\n",
    "        grid_x, grid_y = i\n",
    "        index = grid_x*grid_x_max + grid_y\n",
    "\n",
    "        per_seg_ind.append(index)\n",
    "    return per_seg_ind\n",
    "\n",
    "\n",
    "\n",
    "#CPU\n",
    "def exlude_od_pair(grid_x_max):\n",
    "\n",
    "##############################\n",
    "## consider with only the first and second lines\n",
    "\n",
    "    line0_nei1 = [[8, 1], [9, 2], [10, 2], [11, 3], [11, 4], [11, 5], [12, 6], [12, 7], [12, 8], [13, 9], [13, 10],\n",
    "                  [13, 11], [13, 12], [13, 13]]\n",
    "    line0_nei2 = [[13, 15], [13, 16], [13, 17], [13, 18],[13,19],[13, 20], [12, 21], [12, 22], [12, 23], [12, 24], [12, 25]]\n",
    "    line0_nei3 = [[8, 3], [9, 4], [9, 5], [10, 6], [10, 7], [10, 8], [11, 9], [11, 10], [11, 11], [11, 12], [11, 13]]\n",
    "    line0_nei4 = [[11,15], [11,16], [11,17], [11,18],[11, 19], [11, 20], [10, 21], [10, 22], [10, 23], [10, 24], [10, 25]]\n",
    "\n",
    "\n",
    "#the line1 behine only is without the third and fourth lines\n",
    "    line1_nei1 = [[0, 12], [1, 13], [2, 13], [3, 13], [4, 13], [5, 13], [6, 13], [7, 13], [8, 13], [9, 13], [10, 13], [11, 13]]\n",
    "    line1_nei2 = [[13, 13], [14, 13], [15, 13], [16, 13], [17, 13], [18, 13], [19, 13], [20, 13], [21, 13], [22, 13], [23, 13],\n",
    "                  [24, 13], [25, 13], [26, 13]]\n",
    "\n",
    "    line1_nei3 = [[0, 14], [1, 15], [2, 15], [3, 15], [4, 15], [5, 15], [6, 15], [7, 15], [8, 15], [9, 15], [10, 15], [11, 15]]\n",
    "\n",
    "    line1_nei4 = [[13, 15], [14, 15], [15, 15], [16,15], [17, 15], [18, 15], [19, 15], [20, 15], [21, 15], [22, 15], [23, 15],\n",
    "                  [24, 15], [25, 15], [26, 15]]\n",
    "\n",
    "    segment_list = []\n",
    "    segment_list.append(line0_nei1)\n",
    "    segment_list.append(line0_nei2)\n",
    "    segment_list.append(line0_nei3)\n",
    "    segment_list.append(line0_nei4)\n",
    "\n",
    "\n",
    "    segment_list.append(line1_nei1)\n",
    "    segment_list.append(line1_nei2)\n",
    "    segment_list.append(line1_nei3)\n",
    "    segment_list.append(line1_nei4)\n",
    "\n",
    "    segment_vec_index = []\n",
    "    for per_segment in segment_list:\n",
    "        per_seg_ind = process_segment(per_segment,grid_x_max)\n",
    "        segment_vec_index.append(per_seg_ind)\n",
    "\n",
    "    exclude_pair = []\n",
    "    for per_seg_ind in segment_vec_index:\n",
    "\n",
    "        for i in range(len(per_seg_ind)-1):\n",
    "            for j in range(i+1, len(per_seg_ind)):\n",
    "                per_pair = [per_seg_ind[i], per_seg_ind[j]]\n",
    "                per_pair1 = [per_seg_ind[j], per_seg_ind[i]]\n",
    "\n",
    "                exclude_pair.append(per_pair)\n",
    "                exclude_pair.append(per_pair1)\n",
    "    # exclude_pair1 = list(set(exclude_pair))\n",
    "\n",
    "    return exclude_pair\n",
    "\n",
    "\n",
    "def od_matrix_exclude(od_matirx, exclude_pair):\n",
    "\n",
    "    for per_pair in exclude_pair:\n",
    "        i, j = per_pair\n",
    "\n",
    "        od_matirx[i][j] = 0.0\n",
    "\n",
    "    return od_matirx\n",
    "\n",
    "\n",
    "\n",
    "#CPU\n",
    "def agent_exist_line_pair1(tour_idx_cpu, agent_grid_list, per_line_full_tensor, per_line_station_list):\n",
    "   \n",
    "\n",
    "    satisfied_od_pair = []\n",
    "\n",
    "    agent_line = (tour_idx_cpu - per_line_full_tensor)\n",
    "\n",
    "    intersection_need = (agent_line == 0).nonzero()\n",
    "\n",
    "    if intersection_need.size()[0] == 0:\n",
    "        pass # there is no interaction\n",
    "\n",
    "    else:\n",
    "        interaction_index_mult = intersection_need[:, 1]\n",
    "        interaction_index_list = []\n",
    "        for i in interaction_index_mult:\n",
    "            interaction_index_list.append(agent_grid_list[i])\n",
    "\n",
    "        for i in agent_grid_list:\n",
    "            if i not in interaction_index_list:\n",
    "                for j in per_line_station_list:\n",
    "                    if j not in interaction_index_list:\n",
    "                        per_od_pair = []\n",
    "                        per_od_pair.append(i)\n",
    "                        per_od_pair.append(j)\n",
    "                        satisfied_od_pair.append(per_od_pair)\n",
    "\n",
    "    return satisfied_od_pair # for each element: the agent station is the first\n",
    "\n",
    "#CPU\n",
    "def min_dis_od(satisfied_od_pair, grid_x_max, dis_lim):\n",
    "   \n",
    "    # output--true_satisfied_od_pair: \n",
    "\n",
    "    true_satisfied_od_pair = []\n",
    "\n",
    "    if satisfied_od_pair: # there are interaction stations\n",
    "\n",
    "        satisfied_od_pair_tensor = torch.tensor(satisfied_od_pair)\n",
    "\n",
    "        grid_x_tensor = satisfied_od_pair_tensor // grid_x_max\n",
    "        grid_y_tensor = satisfied_od_pair_tensor % grid_x_max\n",
    "\n",
    "        dis_x = grid_x_tensor[:, 1] - grid_x_tensor[:, 0]\n",
    "        dis_y = grid_y_tensor[:, 1] - grid_y_tensor[:, 0]\n",
    "\n",
    "        dis_tensor = (dis_x.pow(2) + dis_y.pow(2)).float().sqrt()\n",
    "        od_index = (dis_tensor > dis_lim).nonzero()\n",
    "\n",
    "        if od_index.size()[0] == 0:\n",
    "            pass  # there is no satisfied_od pair\n",
    "        else:\n",
    "            satisfied_od_index = od_index[:, 0]\n",
    "\n",
    "            for i in satisfied_od_index:\n",
    "                true_satisfied_od_pair.append(satisfied_od_pair[i])\n",
    "\n",
    "    return true_satisfied_od_pair\n",
    "\n",
    "\n",
    "\n",
    "# CPU\n",
    "def satisfied_od_pair_fn1(tour_idx_cpu, agent_grid_list, line_full_tensor, line_station_list, exist_line_num, grid_x_max, dis_lim):\n",
    "    \n",
    "\n",
    "    satisfied_od_pair1 = agent_pair(agent_grid_list)\n",
    "\n",
    "    if dis_lim == -1: #od pairs in reward only consider agent line\n",
    "        satisfied_od_pair = satisfied_od_pair1\n",
    "\n",
    "    else:\n",
    "\n",
    "        satisfied_od_pair2 = []\n",
    "\n",
    "        for i in range(exist_line_num):\n",
    "            per_line_full_tensor = line_full_tensor[i]\n",
    "\n",
    "            per_line_station_list = line_station_list[i]\n",
    "\n",
    "            per_satisfied_od_pair2 = agent_exist_line_pair1(tour_idx_cpu, agent_grid_list, per_line_full_tensor, per_line_station_list)\n",
    "            \n",
    "\n",
    "            if dis_lim:\n",
    "               \n",
    "                per_true_satisfied_od_pair = min_dis_od(per_satisfied_od_pair2, grid_x_max, dis_lim)\n",
    "\n",
    "                satisfied_od_pair2 = satisfied_od_pair2 + per_true_satisfied_od_pair\n",
    "\n",
    "            else: \n",
    "                satisfied_od_pair2 = satisfied_od_pair2 + per_satisfied_od_pair2\n",
    "\n",
    "        satisfied_od_pair = satisfied_od_pair1 + satisfied_od_pair2\n",
    "\n",
    "    return satisfied_od_pair  # list cpu\n",
    "\n",
    "#CPU\n",
    "def satisfied_od_mask_fn1(grid_num, satisfied_od_pair):\n",
    "    # build the satisfied_od_mask: the element 1(0) present this od is (is not) satisfied by the agent\n",
    "\n",
    "    satisfied_od_mask = torch.zeros(grid_num, grid_num).byte()  # initial########################\n",
    "\n",
    "    value = torch.tensor([1]).byte()\n",
    "\n",
    "    for per_pair in satisfied_od_pair:\n",
    "        i, j = per_pair\n",
    "\n",
    "        satisfied_od_mask[i][j] = value\n",
    "\n",
    "    return satisfied_od_mask\n",
    "\n",
    "#CPU\n",
    "def reward_fn1(tour_idx_cpu, grid_num, agent_grid_list, line_full_tensor, line_station_list, exist_line_num, od_matirx, grid_x_max, dis_lim):\n",
    "\n",
    "    satisfied_od_pair = satisfied_od_pair_fn1(tour_idx_cpu, agent_grid_list, line_full_tensor, line_station_list, exist_line_num, grid_x_max, dis_lim)\n",
    "    # up ok\n",
    "    satisfied_od_mask = satisfied_od_mask_fn1(grid_num, satisfied_od_pair)\n",
    "\n",
    "    satisfied_od_tensor = torch.masked_select(od_matirx, satisfied_od_mask)\n",
    "\n",
    "    reward = satisfied_od_tensor.sum()   # CPU\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#CPU\n",
    "\n",
    "def build_grid_price(path_house, grid_x_max, grid_y_max):\n",
    "#input--path_house: r'/home/weiyu/program/metro_expand_combination/index_average_price.txt'\n",
    "    # price_matrix = torch.zeros((grid_y_max, grid_x_max)).to(device)\n",
    "    price_matrix = torch.zeros((grid_y_max, grid_x_max)).float()\n",
    "\n",
    "    f = open(path_house, 'r')\n",
    "\n",
    "    for line in f:\n",
    "        grid,price = line.rstrip().split('\\t')\n",
    "        index_x,index_y = grid.split(',')\n",
    "\n",
    "        index_x = int(index_x)\n",
    "        index_y = int(index_y)\n",
    "\n",
    "        price_matrix[index_x][index_y] = float(price)\n",
    "    f.close()\n",
    "    return price_matrix\n",
    "\n",
    "# # Utilitarianism\n",
    "def agent_grids_price(tour_idx_cpu, grid_x_max, price_matrix):\n",
    "    agent_grids_num = tour_idx_cpu.size()[1]\n",
    "\n",
    "    grid_x = tour_idx_cpu // grid_x_max\n",
    "    grid_y = tour_idx_cpu % grid_x_max\n",
    "    grid_index = torch.cat((grid_x, grid_y), dim=0)\n",
    "\n",
    "    grids = grid_index.transpose(0, 1)  # torch.int64\n",
    "\n",
    "    tour_idx_price = torch.zeros((agent_grids_num, 1)).float()\n",
    "\n",
    "    for i in range(agent_grids_num):\n",
    "        per_grid = grid_index[:, i]\n",
    "        gridx = per_grid[0]\n",
    "        gridy = per_grid[1]\n",
    "\n",
    "        per_price = price_matrix[gridx, gridy]\n",
    "        tour_idx_price[i][0] = per_price\n",
    "\n",
    "    Ac = []\n",
    "\n",
    "    for i in range(agent_grids_num):\n",
    "        per_grid_expand = grids[i].expand_as(grids)\n",
    "\n",
    "        per_need = per_grid_expand - grids\n",
    "\n",
    "        per_need1 = per_need.pow(2)\n",
    "\n",
    "        per_need2 = per_need1.sum(dim=1).float()\n",
    "\n",
    "        per_need3 = per_need2.sqrt()\n",
    "\n",
    "        per_need4 = torch.exp(-0.5 * per_need3)\n",
    "\n",
    "\n",
    "        per_need5 = per_need4 * (tour_idx_price.transpose(0, 1))\n",
    "\n",
    "        per_Ac = per_need5.sum()\n",
    "\n",
    "        Ac.append(per_Ac)\n",
    "    agent_Ac = sum(Ac)\n",
    "\n",
    "    return agent_Ac\n",
    "\n",
    "# #Equal Sharing\n",
    "def agent_grids_price1(tour_idx_cpu, grid_x_max, price_matrix):\n",
    "    agent_grids_num = tour_idx_cpu.size()[1]\n",
    "\n",
    "    grid_x = tour_idx_cpu // grid_x_max\n",
    "    grid_y = tour_idx_cpu % grid_x_max\n",
    "    grid_index = torch.cat((grid_x, grid_y), dim=0)\n",
    "\n",
    "    grids = grid_index.transpose(0, 1)  # torch.int64\n",
    "\n",
    "    tour_idx_price = torch.zeros((agent_grids_num, 1)).float()\n",
    "\n",
    "    for i in range(agent_grids_num):\n",
    "        per_grid = grid_index[:, i]\n",
    "        gridx = per_grid[0]\n",
    "        gridy = per_grid[1]\n",
    "\n",
    "        per_price = price_matrix[gridx, gridy]\n",
    "        tour_idx_price[i][0] = per_price\n",
    "\n",
    "    Ac = []\n",
    "\n",
    "    for i in range(agent_grids_num):\n",
    "        per_grid_expand = grids[i].expand_as(grids)\n",
    "\n",
    "        per_need = per_grid_expand - grids\n",
    "\n",
    "        per_need1 = per_need.pow(2)\n",
    "\n",
    "        per_need2 = per_need1.sum(dim=1).float()\n",
    "\n",
    "        per_need3 = per_need2.sqrt()\n",
    "\n",
    "        per_need4 = torch.exp(-0.5 * per_need3)\n",
    "\n",
    "        per_need4[i] = 0  # the increase needs exclude the owner.\n",
    "\n",
    "        per_need5 = per_need4 * (tour_idx_price.transpose(0, 1))\n",
    "\n",
    "        per_Ac = per_need5.sum()\n",
    "        per_Ac1 = per_Ac.view(1)\n",
    "\n",
    "        Ac.append(per_Ac1) # Ac example: [tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]\n",
    "\n",
    "    average_Ac = sum(Ac) / agent_grids_num\n",
    "\n",
    "    Ac_tensor = torch.cat(Ac, dim=0)\n",
    "\n",
    "    total_diff_sum = torch.tensor(0.0)\n",
    "    for i in range(agent_grids_num):\n",
    "        per_difference = Ac_tensor[i].view(1)-Ac_tensor\n",
    "        per_diff_abs = torch.abs(per_difference, out = None)\n",
    "        per_diff_sum = per_diff_abs.sum()\n",
    "        total_diff_sum = total_diff_sum + per_diff_sum\n",
    "    try:\n",
    "        pi = math.pi\n",
    "        agent_Ac = total_diff_sum / (2*pi*pi*average_Ac)\n",
    "\n",
    "        # agent_Ac = total_diff_sum / (2 * agent_grids_num * agent_grids_num * average_Ac)\n",
    "        agent_Ac = agent_Ac.data[0]\n",
    "    except: #average_Ac may be 0\n",
    "        agent_Ac = torch.tensor(0.0)\n",
    "    finally:\n",
    "        if torch.isnan(agent_Ac):  \n",
    "            agent_Ac = torch.tensor(0.0)\n",
    "    return agent_Ac\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n",
      "D {'Kari': 'D', 'Lyra': 'B+', 'Aubrey': 'D+', 'Darryl': 'B+', 'Celestine': 'A-', 'Brandi': 'B', 'Wanda': 'C-', 'Pauline': 'D', 'Jacoby': 'F', 'Cecil': 'B+', 'Felicity': 'D', 'Zane': 'A', 'Fabrizia': 'D+', 'Deandra': 'C', 'Zeke': 'C-', 'Kenna': 'C', 'Sherrie': 'B', 'Katherine': 'B-', 'Ulrica': 'A-', 'Lacey': 'B'}\n",
      "dict_items([('Kari', 'D'), ('Lyra', 'B+'), ('Aubrey', 'D+'), ('Darryl', 'B+'), ('Celestine', 'A-'), ('Brandi', 'B'), ('Wanda', 'C-'), ('Pauline', 'D'), ('Jacoby', 'F'), ('Cecil', 'B+'), ('Felicity', 'D'), ('Zane', 'A'), ('Fabrizia', 'D+'), ('Deandra', 'C'), ('Zeke', 'C-'), ('Kenna', 'C'), ('Sherrie', 'B'), ('Katherine', 'B-'), ('Ulrica', 'A-'), ('Lacey', 'B')])\n",
      "<class 'dict'>\n",
      "dict_keys(['Kari', 'Lyra', 'Aubrey', 'Darryl', 'Celestine', 'Brandi', 'Wanda', 'Pauline', 'Jacoby', 'Cecil', 'Felicity', 'Zane', 'Fabrizia', 'Deandra', 'Zeke', 'Kenna', 'Sherrie', 'Katherine', 'Ulrica', 'Lacey'])\n",
      "['Felicity', 'Kari', 'Pauline']\n"
     ]
    }
   ],
   "source": [
    "def get_students(GPA, student_record):\n",
    "    names = [key for key in student_record.keys() if student_record[key] == GPA]\n",
    "    return names\n",
    "GPA = 'D'\n",
    "record =\"{'Kari': 'D', 'Lyra': 'B+', 'Aubrey': 'D+', 'Darryl': 'B+', 'Celestine': 'A-', 'Brandi': 'B', 'Wanda': 'C-', 'Pauline': 'D', 'Jacoby': 'F', 'Cecil': 'B+', 'Felicity': 'D', 'Zane': 'A', 'Fabrizia': 'D+', 'Deandra': 'C', 'Zeke': 'C-', 'Kenna': 'C', 'Sherrie': 'B', 'Katherine': 'B-', 'Ulrica': 'A-', 'Lacey': 'B'}\"\n",
    "\n",
    "print(GPA)\n",
    "\n",
    "student_record=eval(record)\n",
    "print(GPA, student_record)\n",
    "print(student_record.items())\n",
    "print(type(student_record))\n",
    "print(student_record.keys())\n",
    "names = get_students(GPA,student_record)\n",
    "names.sort()\n",
    "print(str(names))\n",
    "GPA='B-'\n",
    "student_record={'Daxton': 'B-', 'Kitty': 'D', 'Angelia': 'C', 'Kimberley': 'D+', 'Bea': 'F', 'Quincy': 'A-', 'Presley': 'A-', 'Preston': 'D+', 'Lyndsey': 'D+', 'Katrina': 'B-', 'Zelida': 'F', 'Douglas': 'A-', 'Lesley': 'B-', 'Primrose': 'C+', 'Brynn': 'A-', 'Monty': 'F', 'Palmer': 'B', 'Darcie': 'D', 'Anthea': 'C-', 'Uriah': 'A'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 9]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = [[1,2],[9,3],[0,5],[4,4]]\n",
    "d = [[3,2],[5,9],[1,2],[9,0]]\n",
    "\n",
    "m = np.column_stack((c[:][0], c[:][1]))\n",
    "\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/weiyu/program/metro_expand_combination/att3/')\n",
    "from metro_vrp import MetroDataset\n",
    "import metro_vrp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('device:',device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##the first step for attention visual, the number of total steps is 4\n",
    "# enc_attn_list = []\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes the static & dynamic states using 1d Convolution.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size): \n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_size, hidden_size, kernel_size=1)\n",
    "\n",
    "    def forward(self, input): \n",
    "        output = self.conv(input)\n",
    "        return output  # (batch, hidden_size, seq_len) \n",
    "\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Calculates attention over the input nodes given the current state.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros((1, hidden_size),\n",
    "                                          device=device, requires_grad=True))\n",
    "        self.V = nn.Parameter(torch.zeros((1, hidden_size),\n",
    "                                          device=device, requires_grad=True))\n",
    "\n",
    "    def forward(self, static_hidden, dynamic_hidden, decoder_hidden):\n",
    "\n",
    "        batch_size, hidden_size, _ = static_hidden.size()\n",
    "\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(2).expand_as(static_hidden)\n",
    "\n",
    "        hidden = decoder_hidden + static_hidden + dynamic_hidden\n",
    "        # if mark is not None:\n",
    "        #     decoder_hidden = np.average(decoder_hidden.squeeze().cpu().detach().numpy())\n",
    "        #     # print(decoder_hidden)\n",
    "        #     static_hidden = np.average(static_hidden.cpu().detach().numpy())\n",
    "        #     dynamic_hidden_d = np.average(dynamic_hidden_d.cpu().detach().numpy())\n",
    "        #     dynamic_hidden_ld = np.average(dynamic_hidden_ld.cpu().detach().numpy())\n",
    "        #     each = [decoder_hidden,static_hidden,dynamic_hidden_d,dynamic_hidden_ld]\n",
    "        #     mark = mark.append([each])\n",
    "        # print(hidden.shape)\n",
    "\n",
    "        # Broadcast some dimensions so we can do batch-matrix-multiply\n",
    "        W = self.W.expand(batch_size, 1,hidden_size )\n",
    "\n",
    "        attns = torch.squeeze(torch.bmm(W, torch.tanh(hidden)),1)\n",
    "\n",
    "        attns = attns\n",
    "\n",
    "        return attns\n",
    "\n",
    "class Pointer(nn.Module):\n",
    "    \"\"\"Calculates the next state given the previous state and input embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super(Pointer, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Used to compute a representation of the current decoder output\n",
    "        self.lstm = torch.nn.LSTMCell(input_size=hidden_size, hidden_size = hidden_size)\n",
    "        self.lstm = self.lstm.to(device)\n",
    "        self.encoder_attn = Attention(hidden_size)\n",
    "        self.encoder_attn = self.encoder_attn.to(device)\n",
    "\n",
    "        self.project_d = nn.Conv1d(hidden_size, hidden_size, kernel_size=1).to(device) #conv1d_1\n",
    "        \n",
    "\n",
    "        self.project_query = nn.Linear(hidden_size, hidden_size).to(device)\n",
    "\n",
    "        self.project_ref = nn.Conv1d(hidden_size, hidden_size, kernel_size=1).to(device) #conv1d_4\n",
    "\n",
    "        self.drop_cc = nn.Dropout(p=dropout)\n",
    "        self.drop_hh = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, static_hidden, dynamic_hidden, decoder_hidden, last_hh,last_cc):\n",
    "\n",
    "        last_hh,last_cc = self.lstm(decoder_hidden, (last_hh,last_cc))\n",
    "\n",
    "\n",
    "        last_hh = self.drop_hh(last_hh)\n",
    "        last_cc = self.drop_hh(last_cc)\n",
    "\n",
    "        static_hidden = self.project_ref(static_hidden)\n",
    "        dynamic_hidden =  self.project_d(dynamic_hidden)\n",
    "        last_hh_1 = self.project_query(last_hh)\n",
    "\n",
    "        enc_attn = self.encoder_attn(static_hidden, dynamic_hidden, last_hh_1)\n",
    "\n",
    "\n",
    "        return enc_attn, last_hh,last_cc\n",
    "\n",
    "\n",
    "class DRL4Metro(nn.Module):  \n",
    "    \"\"\"Defines the main Encoder, Decoder, and Pointer combinatorial models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    static_size: int\n",
    "        Defines how many features are in the static elements of the model\n",
    "        (e.g. 2 for (x, y) coordinates)\n",
    "    dynamic_size: int > 1\n",
    "        Defines how many features are in the dynamic elements of the model\n",
    "        (e.g. 2 for the VRP which has (load, demand) attributes. The TSP doesn't\n",
    "        have dynamic elements, but to ensure compatility with other optimization\n",
    "        problems, assume we just pass in a vector of zeros.\n",
    "    hidden_size: int\n",
    "        Defines the number of units in the hidden layer for all static, dynamic,\n",
    "        and decoder output units.\n",
    "    update_fn: function or None\n",
    "        If provided, this method is used to calculate how the input dynamic\n",
    "        elements are updated, and is called after each 'point' to the input element.\n",
    "    mask_fn: function or None\n",
    "        Allows us to specify which elements of the input sequence are allowed to\n",
    "        be selected. This is useful for speeding up training of the networks,\n",
    "        by providing a sort of 'rules' guidlines to the algorithm. If no mask\n",
    "        is provided, we terminate the search after a fixed number of iterations\n",
    "        to avoid tours that stretch forever\n",
    "    num_layers: int\n",
    "        Specifies the number of hidden layers to use in the decoder RNN\n",
    "    dropout: float\n",
    "        Defines the dropout rate for the decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, static_size, dynamic_size, weight, hidden_size, update_fn = None, mask_fn = None, v_to_g_fn = None,\n",
    "                 vector_allow_fn = None, num_layers=1, dropout=0.):\n",
    "        super(DRL4Metro, self).__init__()\n",
    "\n",
    "        if dynamic_size < 1:\n",
    "            raise ValueError(':param dynamic_size: must be > 0, even if the '\n",
    "                             'problem has no dynamic elements')\n",
    "\n",
    "        self.update_fn = update_fn\n",
    "        self.mask_fn = mask_fn\n",
    "        self.vector_allow_fn = vector_allow_fn\n",
    "        self.v_to_g_fn = v_to_g_fn\n",
    "\n",
    "        # Define the encoder & decoder models\n",
    "        self.static_encoder = Encoder(static_size, hidden_size)\n",
    "        self.dynamic_encoder = Encoder(dynamic_size, hidden_size)\n",
    "        self.decoder = Encoder(static_size, hidden_size)\n",
    "        self.pointer = Pointer(hidden_size, num_layers, dropout)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if len(p.shape) > 1:\n",
    "                nn.init.xavier_uniform_(p)  \n",
    "\n",
    "        # Used as a proxy initial state in the decoder when not specified\n",
    "        self.x0 = torch.zeros((1, static_size, 1), requires_grad=True, device=device)\n",
    "\n",
    "\n",
    "    def forward(self, static, dynamic, station_num_lim, budget =None, initial_direct = None,line_unit_price = None, station_price = None,\n",
    "                decoder_input=None, last_hh=None):\n",
    "        # initial_direct: direction \n",
    "        # line_unit_price: example:  1.0\n",
    "        # station_price: example: 2.0\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        static: Array of size (batch_size, feats, num_cities)\n",
    "            Defines the elements to consider as static. For the TSP, this could be\n",
    "            things like the (x, y) coordinates, which won't change\n",
    "        dynamic: Array of size (batch_size, feats, num_cities)\n",
    "            Defines the elements to consider as static. For the VRP, this can be\n",
    "            things like the (load, demand) of each city. If there are no dynamic\n",
    "            elements, this can be set to None\n",
    "        decoder_input: Array of size (batch_size, num_feats)\n",
    "            Defines the outputs for the decoder. Currently, we just use the\n",
    "            static elements (e.g. (x, y) coordinates), but this can technically\n",
    "            be other things as well\n",
    "        last_hh: Array of size (batch_size, num_hidden)\n",
    "            Defines the last hidden state for the RNN\n",
    "        \"\"\"\n",
    "\n",
    "        def each_line_cost(grid_index1, exist_agent_last_grid):\n",
    "            # this function compute the cost for building each line\n",
    "            need1 = grid_index1 - exist_agent_last_grid\n",
    "            need2 = need1.pow(2)\n",
    "            need3 = need2.sum(dim=1).float()\n",
    "            dis = need3.sqrt().data.cpu().item()\n",
    "            per_line_cost = line_unit_price * dis\n",
    "            return per_line_cost\n",
    "\n",
    "\n",
    "\n",
    "        batch_size, input_size, sequence_size = static.size()\n",
    "\n",
    "        self.direction_vector = torch.zeros((1, 8)).long().to(device)\n",
    "        \n",
    "\n",
    "        if initial_direct: # give the initial direction\n",
    "            for i in initial_direct:\n",
    "                self.direction_vector[0][i] = 1\n",
    "\n",
    "        if budget:\n",
    "            available_fund = budget\n",
    "\n",
    "        if decoder_input is None:\n",
    "            decoder_input = self.x0.expand(batch_size, -1, -1) #decoder_input size (batch,static_size,1)\n",
    "\n",
    "        vector_index_allow = torch.tensor([1])\n",
    "        \n",
    "\n",
    "        specify_original_station = 0  \n",
    "        if dynamic.sum():\n",
    "            specify_original_station = 1\n",
    "\n",
    "            non_zero_index = torch.nonzero(dynamic)\n",
    "            ptr0 = non_zero_index[0][2]\n",
    "            ptr = ptr0.view(1)\n",
    "\n",
    "            grid_index1 = self.v_to_g_fn(ptr.data[0])\n",
    "            agent_current_index = ptr.data.cpu().numpy()[0]\n",
    "            agent_grids = grid_index1\n",
    "            exist_agent_last_grid = grid_index1.view(1, 2)  # grid_x,grid_y\n",
    "\n",
    "            self.direction_vector, vector_index_allow = self.vector_allow_fn(agent_current_index, grid_index1,\n",
    "                                                                         exist_agent_last_grid, self.direction_vector)\n",
    "\n",
    "            decoder_input = static[0, :, agent_current_index]\n",
    "            decoder_input = decoder_input.view(1, 2, 1)   \n",
    "\n",
    "\n",
    "            if self.mask_fn is not None: \n",
    "                if vector_index_allow.size()[0]: \n",
    "                    mask = self.mask_fn(vector_index_allow).detach()\n",
    "                else:\n",
    "                    raise Exception('The initial station is not appropriate!!!')\n",
    "            else:\n",
    "                mask = torch.ones(batch_size, sequence_size, device=device)\n",
    "        else:\n",
    "            # Always use a mask - if no function is provided, we don't update it\n",
    "            mask = torch.ones(batch_size, sequence_size, device=device)\n",
    "            \n",
    "\n",
    "\n",
    "        # Structures for holding the output sequences\n",
    "        tour_idx, tour_logp = [], []\n",
    "        max_steps = sequence_size if self.mask_fn is None else station_num_lim\n",
    "\n",
    "        if specify_original_station:  # add the initial station index\n",
    "            tour_idx.append(ptr.data.unsqueeze(1))\n",
    "\n",
    "        # Static elements only need to be processed once, and can be used across\n",
    "        # all 'pointing' iterations. When / if the dynamic elements change,\n",
    "        # their representations will need to get calculated again.\n",
    "        static_hidden = self.static_encoder(static) #static: Array of size (batch_size, feats, num_cities)\n",
    "        dynamic_hidden = self.dynamic_encoder(dynamic)\n",
    "        \n",
    "        last_hh = torch.zeros((batch_size,dynamic_hidden.size()[1]),device=device,requires_grad= True)      # batch*beam x hidden_size\n",
    "        last_cc = torch.zeros((batch_size, dynamic_hidden.size()[1]), device=device,requires_grad=True)\n",
    "\n",
    "        count_num = 0\n",
    "        for _ in range(max_steps):\n",
    "            count_num = count_num + 1\n",
    "\n",
    "            if vector_index_allow.size()[0] == 0:\n",
    "                break  \n",
    "\n",
    "            if budget:\n",
    "                if available_fund <= 0:\n",
    "                    break\n",
    "\n",
    "            # ... but compute a hidden rep for each element added to sequence\n",
    "            decoder_hidden = self.decoder(decoder_input)\n",
    "            # decoder_input: size (batch,static_size, 1) \n",
    "            # decoder_hidden: size  (batch, hidden_size, 1)\n",
    "            decoder_hidden = torch.squeeze(decoder_hidden, 2)\n",
    "\n",
    "            probs, last_hh,last_cc  = self.pointer(static_hidden,\n",
    "                                          dynamic_hidden,\n",
    "                                          decoder_hidden, last_hh,last_cc)\n",
    "\n",
    "            #probs = F.softmax(probs + mask.log(), dim=1)      # original program\n",
    "            probs = F.softmax(probs + mask*10000, dim=1)\n",
    "            #probs: size (batch,sequence_size) \n",
    "\n",
    "\n",
    "            # When training, sample the next step according to its probability.\n",
    "            # During testing, we can take the greedy approach and choose highest\n",
    "            if self.training:\n",
    "                # print('####################  trainging')\n",
    "                m = torch.distributions.Categorical(probs) \n",
    "\n",
    "                # Sometimes an issue with Categorical & sampling on GPU; See:\n",
    "                # https://github.com/pemami4911/neural-combinatorial-rl-pytorch/issues/5\n",
    "                ptr = m.sample()\n",
    "                \n",
    "                logp = m.log_prob(ptr) #\n",
    "            else:\n",
    "                # print('!!!!!!!!!!!!!!!!!!!!  Greddy')\n",
    "                prob, ptr = torch.max(probs, 1)  # Greedy\n",
    "                logp = prob.log()\n",
    "\n",
    "            # After visiting a node update the dynamic representation\n",
    "            # Change the vector index to grid index\n",
    "            grid_index1 = self.v_to_g_fn(ptr.data[0])   # CUDA  ptr: current grid selected by network\n",
    "            agent_current_index = ptr.data.cpu().numpy()[0] # int\n",
    "\n",
    "            # Got the agent grid index sequence\n",
    "            if count_num == 1 and specify_original_station == 0:\n",
    "                agent_grids = grid_index1\n",
    "                exist_agent_last_grid = grid_index1.view(1, 2)  # grid_x,grid_y\n",
    "            else:\n",
    "                exist_agent_last_grid = agent_grids[-1].view(1, 2) \n",
    "                agent_grids = torch.cat((agent_grids, grid_index1), dim=0)  \n",
    "\n",
    "\n",
    "            self.direction_vector, vector_index_allow = self.vector_allow_fn(agent_current_index, grid_index1, exist_agent_last_grid, self.direction_vector)\n",
    "\n",
    "            tour_logp.append(logp.unsqueeze(1)) # logp.unsqueeze(1) \n",
    "            tour_idx.append(ptr.data.unsqueeze(1)) #ptr.data.unsqueeze(1) \n",
    "\n",
    "            # After visiting a node update the dynamic representation\n",
    "            if self.update_fn is not None:\n",
    "                dynamic = self.update_fn(dynamic, agent_current_index)   # dynamic.requires_grad = False\n",
    "                dynamic_hidden = self.dynamic_encoder(dynamic)\n",
    "\n",
    "                # if count_num == 1:\n",
    "                #     dynamic0 = dynamic.clone()\n",
    "\n",
    "            # And update the mask so we don't re-visit if we don't need to\n",
    "            if self.mask_fn is not None:\n",
    "                if vector_index_allow.size()[0]: \n",
    "                    mask = self.mask_fn(vector_index_allow).detach()\n",
    "\n",
    "            decoder_input = torch.gather(static, 2,\n",
    "                                         ptr.view(-1, 1, 1)\n",
    "                                         .expand(-1, input_size, 1)).detach()\n",
    "            #decoder_input: \n",
    "            # budget\n",
    "            if budget:\n",
    "                per_line_cost = each_line_cost(grid_index1, exist_agent_last_grid)\n",
    "                available_fund = available_fund - per_line_cost - station_price\n",
    "\n",
    "        tour_idx = torch.cat(tour_idx, dim=1)  # (batch_size, seq_len)  tour_idx.requires_grad = False\n",
    "        tour_logp = torch.cat(tour_logp, dim=1)  # (batch_size, seq_len)\n",
    "\n",
    "        return tour_idx, tour_logp\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    raise Exception('Cannot be called from main')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
